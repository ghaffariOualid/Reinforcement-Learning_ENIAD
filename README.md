# Machine Learning II - Reinforcement Learning

**Universit√©** : Mohamed Premier Oujda  
**√âcole** : Nationale de l'Intelligence Artificielle et du Digital Berkane  
**Ann√©e universitaire** : 2024 / 2025  
**Professeur** : [Mohamed Khalifa BOUTAHIR](mailto:email@example.com) 
________________________________________
üìñ Introduction
Ce repository contient une s√©rie de Travaux Pratiques (TP) sur l'apprentissage par renforcement ( Reinforcement Learning - RL). Il couvre les bases des algorithmes tabulaires comme le Q-Learning et SARSA, jusqu'aux m√©thodes plus avanc√©es comme Proximal Policy Optimization (PPO).
Les TP sont con√ßus pour aider les √©tudiants √† comprendre et impl√©menter ces algorithmes dans des environnements simul√©s.
________________________________________

# Table des Mati√®res
1. [TP1 - D√©couverte d'OpenAI Gym](#tp1---d√©couverte-dopenai-gym)  
2. [TP2 - Q-Learning avec FrozenLake](#tp2---q-learning-avec-frozenlake)  
3. [TP3 - Optimisation des Feux de Circulation](#tp3---optimisation-des-feux-de-circulation)  
4. [TP4 - PPO avec Taxi-v3](#tp4---ppo-avec-taxi-v3)  
5. [Structure du Repository](#structure-du-repository)  
6. [Installation](#installation)  
7. [Ressources](#ressources)  
________________________________________
# TP1 - D√©couverte d'OpenAI Gym
Objectifs
‚Ä¢	Se familiariser avec les environnements de Reinforcement Learning
‚Ä¢	Explorer l'environnement CartPole-v1
‚Ä¢	Impl√©menter des politiques al√©atoires
R√©sultats Cl√©s
‚Ä¢	Compr√©hension des espaces d'√©tats et d'actions
‚Ä¢	Performance des actions al√©atoires : ~20 pas avant √©chec
‚Ä¢	Visualisation des √©tats et r√©compenses
________________________________________
 # TP2 - Q-Learning avec FrozenLake
Objectifs
‚Ä¢	Impl√©menter l'algorithme Q-Learning
‚Ä¢	Comprendre l'exploration vs exploitation
‚Ä¢	Analyser la convergence de la Q-table
Algorithmes Cl√©s
Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max Q(s',a') - Q(s,a)]
R√©sultats
M√©trique	Valeur
Taux de r√©ussite (al√©atoire)	1.5%
Taux de r√©ussite (apr√®s Q-Learning)	75%
√âpisodes d'entra√Ænement	5000
________________________________________
# TP3 - Optimisation des Feux de Circulation
Comparaison Q-Learning vs SARSA
# Q-Learning (off-policy)
Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max Q(s',a') - Q(s,a)]

# SARSA (on-policy)
Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ Q(s',a') - Q(s,a)]
Performances
Algorithme	R√©duction Temps d'Attente
Q-Learning	82%
SARSA	78%
________________________________________
# TP4 - PPO avec Taxi-v3
Proximal Policy Optimization
Fonction objectif avec clipping :
L(Œ∏) = ·µú[min(r_t(Œ∏)A_t, clip(r_t(Œ∏), 1-Œµ, 1+Œµ)A_t)]
R√©sultats
Phase	Taux de R√©ussite	Steps Moyens
Avant entra√Ænement	0%	200+
Apr√®s 1000 √©pisodes	92%	15.2
________________________________________
üìÇ Structure du Repository
ML2/
‚îú‚îÄ‚îÄ TP1/                  # D√©couverte OpenAI Gym
‚îú‚îÄ‚îÄ TP2/                  # Q-Learning FrozenLake
‚îú‚îÄ‚îÄ TP3/                  # Feux de Circulation
‚îú‚îÄ‚îÄ TP4/                  # PPO Taxi-v3
‚îú‚îÄ‚îÄ requirements.txt       # D√©pendances
‚îî‚îÄ‚îÄ README.md             # Ce fichier
________________________________________
üõ†Ô∏è Installation
Pr√©requis
‚Ä¢	Python 3.x
‚Ä¢	pip install√©
Installation
1.	Cloner le repository :
git clone https://github.com/votre-utilisateur/ML2.git
2.	Cr√©er un environnement virtuel et installer les d√©pendances :
cd ML2
python -m venv venv
source venv/bin/activate  # (ou venv\Scripts\activate sous Windows)
pip install -r requirements.txt
3.	Ex√©cuter un TP :
python tp1.py
________________________________________
# Contributions
Les contributions sont les bienvenues ! Pour proposer une modification :
1.	Forker le repository
2.	Cr√©er une branche avec un nom explicite :
git checkout -b feature-nouvelle-fonctionnalite
3.	Faire un commit et push :
git commit -m "Ajout d'une nouvelle fonctionnalit√©"
git push origin feature-nouvelle-fonctionnalite
4.	Ouvrir une Pull Request sur GitHub.
________________________________________
# Ressources
Documentation :
‚Ä¢	OpenAI Gymnasium
‚Ä¢	Stable Baselines3
Livres :
‚Ä¢	"Reinforcement Learning: An Introduction" - Sutton & Barto
‚Ä¢	"Deep Reinforcement Learning Hands-On" - Maxim Lapan
Articles :
‚Ä¢	Proximal Policy Optimization (PPO) - Schulman et al. 2017
‚Ä¢	Q-Learning Convergence - Watkins & Dayan 1992
________________________________________
üìù Remarques Finales
Ce repository couvre les fondamentaux du Reinforcement Learning :
‚Ä¢	Des algorithmes tabulaires (Q-Learning, SARSA)
‚Ä¢	Aux m√©thodes de politique avanc√©es (PPO)
‚Ä¢	Avec applications sur des probl√®mes concrets
‚Ä¢	Les visualisations et comparaisons permettent de bien comprendre les forces/faiblesses de chaque approche.


